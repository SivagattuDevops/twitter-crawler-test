{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6b99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import gzip\n",
    "import os\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize   \n",
    "from nltk.corpus import stopwords         \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "from nltk.stem import PorterStemmer      \n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3336dc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Arya\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Arya\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Arya Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Arya\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Arya\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83329833",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./tweets/\"\n",
    "process_root = \"./tweets_process/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6267fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(process_root):\n",
    "        os.makedirs(process_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2cc905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(text):\n",
    "    res = []\n",
    "    text = text.strip('RT')\n",
    "    at_pattern = re.compile(\"@[\\S]*\")\n",
    "    url_pattern = re.compile(\"https:\\/\\/[\\w./]*[\\w/]\")\n",
    "    text = re.sub(at_pattern,'',text)\n",
    "    text = re.sub(url_pattern,'',text)\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(list(string.punctuation))\n",
    "    filtered = [w for w in tokens if w not in stop_words]\n",
    "    lemmatizaer = WordNetLemmatizer()\n",
    "    for word, tag in nltk.pos_tag(filtered):\n",
    "        if tag.startswith('NN'):\n",
    "            word= lemmatizaer.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            word= lemmatizaer.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            word= lemmatizaer.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            word= lemmatizaer.lemmatize(word, pos='r')\n",
    "        res.append(word)\n",
    "    #porter_stemmer = PorterStemmer()\n",
    "    #for word in lemmatized_word:\n",
    "        #res.append(porter_stemmer.stem(word))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dd3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_process(file):\n",
    "    df = pd.read_csv(root+file)\n",
    "    df['text_process'] = df['full_text'].apply(lambda x: \" \".join(text_process(x)))\n",
    "    df = df.loc[:,['id','text_process']]\n",
    "    df.to_csv(process_root+file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55905a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 905/905 [02:31<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(root):\n",
    "    for i in tqdm(range(len(files))):\n",
    "        file = files[i]\n",
    "        csv_process(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5f78a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
